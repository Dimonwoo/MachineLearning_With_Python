{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging-Boosting-Voting\n",
    "\n",
    "* The three most popular methods for combining the predictions from different models are:\n",
    "\n",
    "    * BAGGING : Building multiple models (typically of the same type) from different subsamples of the training dataset.\n",
    "    * BOOSTING : Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the chain.\n",
    "    * VOTING : Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "dataset=make_moons(n_samples=10000, shuffle=True, noise=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61709259, -0.04290016],\n",
       "       [-0.42796479, -0.11405297],\n",
       "       [ 0.36319517,  0.69701942],\n",
       "       ...,\n",
       "       [-0.10594722,  0.2334626 ],\n",
       "       [ 0.88213375,  0.53205719],\n",
       "       [ 1.47232119, -0.27006222]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOTING :\n",
    "* Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions.\n",
    "\n",
    "* **Voting Classifier** Voting classifier has two types. Hard Voting Classifier and Soft Voting Classifier\n",
    "* Voating Classicier works in Ensemble or RandomForest or in Deep Learning.\n",
    "* Suppose in Random Forest classifier has two classes ***0*** and ***1*** and we have four Decision Trees **[D1,D2,D3,D4]** which gives the results- **[0, 1, 1, 1]**.\n",
    "    * For **Hard Voting Classifier** the result would be ***1*** since the maximum models gives the output.\n",
    "    * For **Soft Voting Classifier** it gives us the probablity. As shown in the below table, It take sthe higher probability scores.\n",
    "    \n",
    "| D | **1** | **0** |\n",
    "|----|-------|-------|\n",
    "| D1 | .95 | .05 |\n",
    "| D2 | .86 | .14 |\n",
    "| D3 | .7 | .3 |\n",
    "| D4 | .6 | .4 |\n",
    "\n",
    "\n",
    "* The result will be (.95+.86+.7+.6)/4 as it is huigher than the other one.\n",
    "\n",
    "![Hard Voting](voting.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ariji\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ariji\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\ariji\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)), ('rf', RandomFo...f', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=None, voting='hard', weights=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "\n",
    "\n",
    "voting_clf = VotingClassifier(estimators = [('lr',log_clf),('rf',rnd_clf),('svm',svm_clf)],voting='hard')\n",
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ariji\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ariji\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\ariji\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8895\n",
      "RandomForestClassifier 0.971\n",
      "SVC 0.9755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ariji\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ariji\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 0.9725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging( Bootstrap Aggrigation):\n",
    "\n",
    "* One of the technique we use for Bagging is Random Forest\n",
    "* Another approach is to use the same training algorithm for every predictor, but to train them on different random subsets of the training set. \n",
    "* When sampling is performed with replacement(Row sampling with replacement), this method is called bagging (short for bootstrap aggregating). \n",
    "* When sampling is performed without replacement, it is called pasting.\n",
    "* When the results come from each models we use Voting on it to get the final result.\n",
    "* The BaggingClassifier automatically performs soft voting instead of hard voting.\n",
    "\n",
    "\n",
    "\n",
    "![bootstapping.png](bootstapping.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators=500,\n",
    "max_samples=100, bootstrap=True, n_jobs=-1\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier 0.9705\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(bag_clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "\n",
    "* Boosting (originally called hypothesis boosting) refers to any Ensemble method that can combine several weak learners into a strong learner. \n",
    "* **The general idea of most boosting methods is to train predictors(base learnears) sequentially, each trying to correct its predecessor.**\n",
    "* There are many boosting methods available, but by far the most popular are **AdaBoost** (short for Adaptive Boosting) and **Gradient Boosting**.\n",
    "\n",
    "![boosting.png](boosting.png)\n",
    "\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "* It consistes of Random Forest having Decision Trees with depth one (**one root with two leaves nodes**) \n",
    "* which is called **Stumps**(**Forest of Stumps**)\n",
    "\n",
    "* The formula of **Sample Weight** is $W=\\frac {1}{n}$; Here $W=\\frac {1}{7}$               \n",
    "* Where  $n$ is the number of record\n",
    "\n",
    "Step 2=\n",
    "$$\n",
    "\\begin{array}\n",
    "  &f1& f2& f3 & O/p&Sample Weights& \\\\\n",
    "  \\hline\n",
    "  0&...&...&...&Yes&1/7\\\\\n",
    "  1&...&...&...&No&1/7\\\\\n",
    "  2&...&...&...&Yes&1/7\\\\\n",
    "  3&...&...&...&No&1/7\\\\\n",
    "  4&...&...&...&No&1/7\\\\\n",
    "  5&...&...&...&+&1/7\\\\\n",
    "  6&...&...&...&+&1/7\\\\ \n",
    " \\end{array}\n",
    "$$\n",
    "\n",
    "* Suppose record no 3 has been wrongly classified. 4 records result was correct and 1 was wrong.\n",
    "* $Total Error(TE)= \\frac {1}{7}$ \n",
    "* $Total Error(TE)= \\frac {Number Of Errors}{Total Sample Weights}$ \n",
    "\n",
    "Step 3=\n",
    "* **Performence of the Stump** = $\\frac{1}{2}\\log_e \\Big( \\frac {1-TE}{TE}\\Big)$\n",
    "\n",
    "$=$ $\\frac{1}{2}\\log_e \\Big( \\frac {1-\\frac{1}{7}}{\\frac{1}{7}}\\Big)$\n",
    "\n",
    "$=$ $\\frac{1}{2}\\log_e \\Big(6\\Big)$\n",
    "\n",
    "$\\therefore$ Performence Say =$0.895$ \n",
    "\n",
    "\n",
    "\n",
    "* **New Sample Weight**=\n",
    "$Weight\\times  e^{Perf Say}$\n",
    "\n",
    "$=\\frac{1}{7}\\times e^{0.895}$\n",
    "\n",
    "$\\therefore The Output=0.349$\n",
    "\n",
    "\n",
    "* In this point we need to update the weights and the formula like - $Weight\\times  e^{-Pref Say}$\n",
    "$$\n",
    "\\begin{array}\n",
    "  &&f1& f2& f3 & O/p&Sample Weights&Updated Weights&\\\\\n",
    "  \\hline\n",
    "  0&...&...&...&Yes&1/7&0.05\\\\\n",
    "  1&...&...&...&No&1/7&0.05\\\\\n",
    "  2&...&...&...&Yes&1/7&0.05\\\\\n",
    "  3&...&...&...&No&1/7&0.349\\\\\n",
    "  4&...&...&...&No&1/7&0.05\\\\\n",
    "  5&...&...&...&+&1/7&0.05\\\\\n",
    "  6&...&...&...&+&1/7&0.05\\\\ \n",
    " \\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\New_Course\\\\python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
